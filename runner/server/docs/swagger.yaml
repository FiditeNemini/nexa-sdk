basePath: /v1
definitions:
  handler.ChatCompletionRequest:
    type: object
  handler.RerankingRequest:
    type: object
  openai.ChatCompletion:
    properties:
      choices:
        description: |-
          A list of chat completion choices. Can be more than one if `n` is greater
          than 1.
        items:
          $ref: '#/definitions/openai.ChatCompletionChoice'
        type: array
      created:
        description: The Unix timestamp (in seconds) of when the chat completion was
          created.
        type: integer
      id:
        description: A unique identifier for the chat completion.
        type: string
      model:
        description: The model used for the chat completion.
        type: string
      object:
        description: The object type, which is always `chat.completion`.
        type: string
      service_tier:
        allOf:
        - $ref: '#/definitions/openai.ChatCompletionServiceTier'
        description: |-
          Specifies the latency tier to use for processing the request. This parameter is
          relevant for customers subscribed to the scale tier service:

            - If set to 'auto', and the Project is Scale tier enabled, the system will
              utilize scale tier credits until they are exhausted.
            - If set to 'auto', and the Project is not Scale tier enabled, the request will
              be processed using the default service tier with a lower uptime SLA and no
              latency guarantee.
            - If set to 'default', the request will be processed using the default service
              tier with a lower uptime SLA and no latency guarantee.
            - If set to 'flex', the request will be processed with the Flex Processing
              service tier.
              [Learn more](https://platform.openai.com/docs/guides/flex-processing).
            - When not set, the default behavior is 'auto'.

          When this parameter is set, the response body will include the `service_tier`
          utilized.

          Any of "auto", "default", "flex", "scale".
      system_fingerprint:
        description: |-
          This fingerprint represents the backend configuration that the model runs with.

          Can be used in conjunction with the `seed` request parameter to understand when
          backend changes have been made that might impact determinism.
        type: string
      usage:
        allOf:
        - $ref: '#/definitions/openai.CompletionUsage'
        description: Usage statistics for the completion request.
    type: object
  openai.ChatCompletionAudio:
    properties:
      data:
        description: |-
          Base64 encoded audio bytes generated by the model, in the format specified in
          the request.
        type: string
      expires_at:
        description: |-
          The Unix timestamp (in seconds) for when this audio response will no longer be
          accessible on the server for use in multi-turn conversations.
        type: integer
      id:
        description: Unique identifier for this audio response.
        type: string
      transcript:
        description: Transcript of the audio generated by the model.
        type: string
    type: object
  openai.ChatCompletionChoice:
    properties:
      finish_reason:
        description: |-
          The reason the model stopped generating tokens. This will be `stop` if the model
          hit a natural stop point or a provided stop sequence, `length` if the maximum
          number of tokens specified in the request was reached, `content_filter` if
          content was omitted due to a flag from our content filters, `tool_calls` if the
          model called a tool, or `function_call` (deprecated) if the model called a
          function.

          Any of "stop", "length", "tool_calls", "content_filter", "function_call".
        type: string
      index:
        description: The index of the choice in the list of choices.
        type: integer
      logprobs:
        allOf:
        - $ref: '#/definitions/openai.ChatCompletionChoiceLogprobs'
        description: Log probability information for the choice.
      message:
        allOf:
        - $ref: '#/definitions/openai.ChatCompletionMessage'
        description: A chat completion message generated by the model.
    type: object
  openai.ChatCompletionChoiceLogprobs:
    properties:
      content:
        description: A list of message content tokens with log probability information.
        items:
          $ref: '#/definitions/openai.ChatCompletionTokenLogprob'
        type: array
      refusal:
        description: A list of message refusal tokens with log probability information.
        items:
          $ref: '#/definitions/openai.ChatCompletionTokenLogprob'
        type: array
    type: object
  openai.ChatCompletionMessage:
    properties:
      annotations:
        description: |-
          Annotations for the message, when applicable, as when using the
          [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
        items:
          $ref: '#/definitions/openai.ChatCompletionMessageAnnotation'
        type: array
      audio:
        allOf:
        - $ref: '#/definitions/openai.ChatCompletionAudio'
        description: |-
          If the audio output modality is requested, this object contains data about the
          audio response from the model.
          [Learn more](https://platform.openai.com/docs/guides/audio).
      content:
        description: The contents of the message.
        type: string
      function_call:
        allOf:
        - $ref: '#/definitions/openai.ChatCompletionMessageFunctionCall'
        description: |-
          Deprecated and replaced by `tool_calls`. The name and arguments of a function
          that should be called, as generated by the model.

          Deprecated: deprecated
      refusal:
        description: The refusal message generated by the model.
        type: string
      role:
        description: The role of the author of this message.
        type: string
      tool_calls:
        description: The tool calls generated by the model, such as function calls.
        items:
          $ref: '#/definitions/openai.ChatCompletionMessageToolCall'
        type: array
    type: object
  openai.ChatCompletionMessageAnnotation:
    properties:
      type:
        description: The type of the URL citation. Always `url_citation`.
        type: string
      url_citation:
        allOf:
        - $ref: '#/definitions/openai.ChatCompletionMessageAnnotationURLCitation'
        description: A URL citation when using web search.
    type: object
  openai.ChatCompletionMessageAnnotationURLCitation:
    properties:
      end_index:
        description: The index of the last character of the URL citation in the message.
        type: integer
      start_index:
        description: The index of the first character of the URL citation in the message.
        type: integer
      title:
        description: The title of the web resource.
        type: string
      url:
        description: The URL of the web resource.
        type: string
    type: object
  openai.ChatCompletionMessageFunctionCall:
    properties:
      arguments:
        description: |-
          The arguments to call the function with, as generated by the model in JSON
          format. Note that the model does not always generate valid JSON, and may
          hallucinate parameters not defined by your function schema. Validate the
          arguments in your code before calling your function.
        type: string
      name:
        description: The name of the function to call.
        type: string
    type: object
  openai.ChatCompletionMessageToolCall:
    properties:
      function:
        allOf:
        - $ref: '#/definitions/openai.ChatCompletionMessageToolCallFunction'
        description: The function that the model called.
      id:
        description: The ID of the tool call.
        type: string
      type:
        description: The type of the tool. Currently, only `function` is supported.
        type: string
    type: object
  openai.ChatCompletionMessageToolCallFunction:
    properties:
      arguments:
        description: |-
          The arguments to call the function with, as generated by the model in JSON
          format. Note that the model does not always generate valid JSON, and may
          hallucinate parameters not defined by your function schema. Validate the
          arguments in your code before calling your function.
        type: string
      name:
        description: The name of the function to call.
        type: string
    type: object
  openai.ChatCompletionServiceTier:
    enum:
    - auto
    - default
    - flex
    - scale
    type: string
    x-enum-varnames:
    - ChatCompletionServiceTierAuto
    - ChatCompletionServiceTierDefault
    - ChatCompletionServiceTierFlex
    - ChatCompletionServiceTierScale
  openai.ChatCompletionStreamOptionsParam:
    properties:
      any: {}
      include_usage:
        allOf:
        - $ref: '#/definitions/param.Opt-bool'
        description: |-
          If set, an additional chunk will be streamed before the `data: [DONE]` message.
          The `usage` field on this chunk shows the token usage statistics for the entire
          request, and the `choices` field will always be an empty array.

          All other chunks will also include a `usage` field, but with a null value.
          **NOTE:** If the stream is interrupted, you may not receive the final usage
          chunk which contains the total token usage for the request.
    type: object
  openai.ChatCompletionTokenLogprob:
    properties:
      bytes:
        description: |-
          A list of integers representing the UTF-8 bytes representation of the token.
          Useful in instances where characters are represented by multiple tokens and
          their byte representations must be combined to generate the correct text
          representation. Can be `null` if there is no bytes representation for the token.
        items:
          type: integer
        type: array
      logprob:
        description: |-
          The log probability of this token, if it is within the top 20 most likely
          tokens. Otherwise, the value `-9999.0` is used to signify that the token is very
          unlikely.
        type: number
      token:
        description: The token.
        type: string
      top_logprobs:
        description: |-
          List of the most likely tokens and their log probability, at this token
          position. In rare cases, there may be fewer than the number of requested
          `top_logprobs` returned.
        items:
          $ref: '#/definitions/openai.ChatCompletionTokenLogprobTopLogprob'
        type: array
    type: object
  openai.ChatCompletionTokenLogprobTopLogprob:
    properties:
      bytes:
        description: |-
          A list of integers representing the UTF-8 bytes representation of the token.
          Useful in instances where characters are represented by multiple tokens and
          their byte representations must be combined to generate the correct text
          representation. Can be `null` if there is no bytes representation for the token.
        items:
          type: integer
        type: array
      logprob:
        description: |-
          The log probability of this token, if it is within the top 20 most likely
          tokens. Otherwise, the value `-9999.0` is used to signify that the token is very
          unlikely.
        type: number
      token:
        description: The token.
        type: string
    type: object
  openai.Completion:
    properties:
      choices:
        description: The list of completion choices the model generated for the input
          prompt.
        items:
          $ref: '#/definitions/openai.CompletionChoice'
        type: array
      created:
        description: The Unix timestamp (in seconds) of when the completion was created.
        type: integer
      id:
        description: A unique identifier for the completion.
        type: string
      model:
        description: The model used for completion.
        type: string
      object:
        description: The object type, which is always "text_completion"
        type: string
      system_fingerprint:
        description: |-
          This fingerprint represents the backend configuration that the model runs with.

          Can be used in conjunction with the `seed` request parameter to understand when
          backend changes have been made that might impact determinism.
        type: string
      usage:
        allOf:
        - $ref: '#/definitions/openai.CompletionUsage'
        description: Usage statistics for the completion request.
    type: object
  openai.CompletionChoice:
    properties:
      finish_reason:
        allOf:
        - $ref: '#/definitions/openai.CompletionChoiceFinishReason'
        description: |-
          The reason the model stopped generating tokens. This will be `stop` if the model
          hit a natural stop point or a provided stop sequence, `length` if the maximum
          number of tokens specified in the request was reached, or `content_filter` if
          content was omitted due to a flag from our content filters.

          Any of "stop", "length", "content_filter".
      index:
        type: integer
      logprobs:
        $ref: '#/definitions/openai.CompletionChoiceLogprobs'
      text:
        type: string
    type: object
  openai.CompletionChoiceFinishReason:
    enum:
    - stop
    - length
    - content_filter
    type: string
    x-enum-varnames:
    - CompletionChoiceFinishReasonStop
    - CompletionChoiceFinishReasonLength
    - CompletionChoiceFinishReasonContentFilter
  openai.CompletionChoiceLogprobs:
    properties:
      text_offset:
        items:
          type: integer
        type: array
      token_logprobs:
        items:
          type: number
        type: array
      tokens:
        items:
          type: string
        type: array
      top_logprobs:
        items:
          additionalProperties:
            type: number
          type: object
        type: array
    type: object
  openai.CompletionNewParams:
    properties:
      any: {}
      best_of:
        allOf:
        - $ref: '#/definitions/param.Opt-int64'
        description: |-
          Generates `best_of` completions server-side and returns the "best" (the one with
          the highest log probability per token). Results cannot be streamed.

          When used with `n`, `best_of` controls the number of candidate completions and
          `n` specifies how many to return – `best_of` must be greater than `n`.

          **Note:** Because this parameter generates many completions, it can quickly
          consume your token quota. Use carefully and ensure that you have reasonable
          settings for `max_tokens` and `stop`.
      echo:
        allOf:
        - $ref: '#/definitions/param.Opt-bool'
        description: Echo back the prompt in addition to the completion
      frequency_penalty:
        allOf:
        - $ref: '#/definitions/param.Opt-float64'
        description: |-
          Number between -2.0 and 2.0. Positive values penalize new tokens based on their
          existing frequency in the text so far, decreasing the model's likelihood to
          repeat the same line verbatim.

          [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
      logit_bias:
        additionalProperties:
          type: integer
        description: |-
          Modify the likelihood of specified tokens appearing in the completion.

          Accepts a JSON object that maps tokens (specified by their token ID in the GPT
          tokenizer) to an associated bias value from -100 to 100. You can use this
          [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.
          Mathematically, the bias is added to the logits generated by the model prior to
          sampling. The exact effect will vary per model, but values between -1 and 1
          should decrease or increase likelihood of selection; values like -100 or 100
          should result in a ban or exclusive selection of the relevant token.

          As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token
          from being generated.
        type: object
      logprobs:
        allOf:
        - $ref: '#/definitions/param.Opt-int64'
        description: |-
          Include the log probabilities on the `logprobs` most likely output tokens, as
          well the chosen tokens. For example, if `logprobs` is 5, the API will return a
          list of the 5 most likely tokens. The API will always return the `logprob` of
          the sampled token, so there may be up to `logprobs+1` elements in the response.

          The maximum value for `logprobs` is 5.
      max_tokens:
        allOf:
        - $ref: '#/definitions/param.Opt-int64'
        description: |-
          The maximum number of [tokens](/tokenizer) that can be generated in the
          completion.

          The token count of your prompt plus `max_tokens` cannot exceed the model's
          context length.
          [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
          for counting tokens.
      model:
        allOf:
        - $ref: '#/definitions/openai.CompletionNewParamsModel'
        description: |-
          ID of the model to use. You can use the
          [List models](https://platform.openai.com/docs/api-reference/models/list) API to
          see all of your available models, or see our
          [Model overview](https://platform.openai.com/docs/models) for descriptions of
          them.
      "n":
        allOf:
        - $ref: '#/definitions/param.Opt-int64'
        description: |-
          How many completions to generate for each prompt.

          **Note:** Because this parameter generates many completions, it can quickly
          consume your token quota. Use carefully and ensure that you have reasonable
          settings for `max_tokens` and `stop`.
      presence_penalty:
        allOf:
        - $ref: '#/definitions/param.Opt-float64'
        description: |-
          Number between -2.0 and 2.0. Positive values penalize new tokens based on
          whether they appear in the text so far, increasing the model's likelihood to
          talk about new topics.

          [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)
      prompt:
        allOf:
        - $ref: '#/definitions/openai.CompletionNewParamsPromptUnion'
        description: |-
          The prompt(s) to generate completions for, encoded as a string, array of
          strings, array of tokens, or array of token arrays.

          Note that <|endoftext|> is the document separator that the model sees during
          training, so if a prompt is not specified the model will generate as if from the
          beginning of a new document.
      seed:
        allOf:
        - $ref: '#/definitions/param.Opt-int64'
        description: |-
          If specified, our system will make a best effort to sample deterministically,
          such that repeated requests with the same `seed` and parameters should return
          the same result.

          Determinism is not guaranteed, and you should refer to the `system_fingerprint`
          response parameter to monitor changes in the backend.
      stop:
        allOf:
        - $ref: '#/definitions/openai.CompletionNewParamsStopUnion'
        description: |-
          Not supported with latest reasoning models `o3` and `o4-mini`.

          Up to 4 sequences where the API will stop generating further tokens. The
          returned text will not contain the stop sequence.
      stream_options:
        allOf:
        - $ref: '#/definitions/openai.ChatCompletionStreamOptionsParam'
        description: 'Options for streaming response. Only set this when you set `stream:
          true`.'
      suffix:
        allOf:
        - $ref: '#/definitions/param.Opt-string'
        description: |-
          The suffix that comes after a completion of inserted text.

          This parameter is only supported for `gpt-3.5-turbo-instruct`.
      temperature:
        allOf:
        - $ref: '#/definitions/param.Opt-float64'
        description: |-
          What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
          make the output more random, while lower values like 0.2 will make it more
          focused and deterministic.

          We generally recommend altering this or `top_p` but not both.
      top_p:
        allOf:
        - $ref: '#/definitions/param.Opt-float64'
        description: |-
          An alternative to sampling with temperature, called nucleus sampling, where the
          model considers the results of the tokens with top_p probability mass. So 0.1
          means only the tokens comprising the top 10% probability mass are considered.

          We generally recommend altering this or `temperature` but not both.
      user:
        allOf:
        - $ref: '#/definitions/param.Opt-string'
        description: |-
          A unique identifier representing your end-user, which can help OpenAI to monitor
          and detect abuse.
          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
    type: object
  openai.CompletionNewParamsModel:
    enum:
    - gpt-3.5-turbo-instruct
    - davinci-002
    - babbage-002
    type: string
    x-enum-varnames:
    - CompletionNewParamsModelGPT3_5TurboInstruct
    - CompletionNewParamsModelDavinci002
    - CompletionNewParamsModelBabbage002
  openai.CompletionNewParamsPromptUnion:
    properties:
      any: {}
      ofArrayOfStrings:
        items:
          type: string
        type: array
      ofArrayOfTokenArrays:
        items:
          items:
            type: integer
          type: array
        type: array
      ofArrayOfTokens:
        items:
          type: integer
        type: array
      ofString:
        $ref: '#/definitions/param.Opt-string'
    type: object
  openai.CompletionNewParamsStopUnion:
    properties:
      any: {}
      ofString:
        $ref: '#/definitions/param.Opt-string'
      ofStringArray:
        items:
          type: string
        type: array
    type: object
  openai.CompletionUsage:
    properties:
      completion_tokens:
        description: Number of tokens in the generated completion.
        type: integer
      completion_tokens_details:
        allOf:
        - $ref: '#/definitions/openai.CompletionUsageCompletionTokensDetails'
        description: Breakdown of tokens used in a completion.
      prompt_tokens:
        description: Number of tokens in the prompt.
        type: integer
      prompt_tokens_details:
        allOf:
        - $ref: '#/definitions/openai.CompletionUsagePromptTokensDetails'
        description: Breakdown of tokens used in the prompt.
      total_tokens:
        description: Total number of tokens used in the request (prompt + completion).
        type: integer
    type: object
  openai.CompletionUsageCompletionTokensDetails:
    properties:
      accepted_prediction_tokens:
        description: |-
          When using Predicted Outputs, the number of tokens in the prediction that
          appeared in the completion.
        type: integer
      audio_tokens:
        description: Audio input tokens generated by the model.
        type: integer
      reasoning_tokens:
        description: Tokens generated by the model for reasoning.
        type: integer
      rejected_prediction_tokens:
        description: |-
          When using Predicted Outputs, the number of tokens in the prediction that did
          not appear in the completion. However, like reasoning tokens, these tokens are
          still counted in the total completion tokens for purposes of billing, output,
          and context window limits.
        type: integer
    type: object
  openai.CompletionUsagePromptTokensDetails:
    properties:
      audio_tokens:
        description: Audio input tokens present in the prompt.
        type: integer
      cached_tokens:
        description: Cached tokens present in the prompt.
        type: integer
    type: object
  openai.EmbeddingModel:
    enum:
    - text-embedding-ada-002
    - text-embedding-3-small
    - text-embedding-3-large
    type: string
    x-enum-varnames:
    - EmbeddingModelTextEmbeddingAda002
    - EmbeddingModelTextEmbedding3Small
    - EmbeddingModelTextEmbedding3Large
  openai.EmbeddingNewParams:
    properties:
      any: {}
      dimensions:
        allOf:
        - $ref: '#/definitions/param.Opt-int64'
        description: |-
          The number of dimensions the resulting output embeddings should have. Only
          supported in `text-embedding-3` and later models.
      encoding_format:
        allOf:
        - $ref: '#/definitions/openai.EmbeddingNewParamsEncodingFormat'
        description: |-
          The format to return the embeddings in. Can be either `float` or
          [`base64`](https://pypi.org/project/pybase64/).

          Any of "float", "base64".
      input:
        allOf:
        - $ref: '#/definitions/openai.EmbeddingNewParamsInputUnion'
        description: |-
          Input text to embed, encoded as a string or array of tokens. To embed multiple
          inputs in a single request, pass an array of strings or array of token arrays.
          The input must not exceed the max input tokens for the model (8192 tokens for
          all embedding models), cannot be an empty string, and any array must be 2048
          dimensions or less.
          [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
          for counting tokens. In addition to the per-input token limit, all embedding
          models enforce a maximum of 300,000 tokens summed across all inputs in a single
          request.
      model:
        allOf:
        - $ref: '#/definitions/openai.EmbeddingModel'
        description: |-
          ID of the model to use. You can use the
          [List models](https://platform.openai.com/docs/api-reference/models/list) API to
          see all of your available models, or see our
          [Model overview](https://platform.openai.com/docs/models) for descriptions of
          them.
      user:
        allOf:
        - $ref: '#/definitions/param.Opt-string'
        description: |-
          A unique identifier representing your end-user, which can help OpenAI to monitor
          and detect abuse.
          [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
    type: object
  openai.EmbeddingNewParamsEncodingFormat:
    enum:
    - float
    - base64
    type: string
    x-enum-varnames:
    - EmbeddingNewParamsEncodingFormatFloat
    - EmbeddingNewParamsEncodingFormatBase64
  openai.EmbeddingNewParamsInputUnion:
    properties:
      any: {}
      ofArrayOfStrings:
        items:
          type: string
        type: array
      ofArrayOfTokenArrays:
        items:
          items:
            type: integer
          type: array
        type: array
      ofArrayOfTokens:
        items:
          type: integer
        type: array
      ofString:
        $ref: '#/definitions/param.Opt-string'
    type: object
  param.Opt-bool:
    properties:
      value:
        type: boolean
    type: object
  param.Opt-float64:
    properties:
      value:
        type: number
    type: object
  param.Opt-int64:
    properties:
      value:
        type: integer
    type: object
  param.Opt-string:
    properties:
      value:
        type: string
    type: object
info:
  contact: {}
  title: Nexa AI Server
  version: 0.0.0
paths:
  /chat/completions:
    post:
      consumes:
      - application/json
      description: This endpoint generates a model response for a given conversation,
        which can include text and images. It supports both single-turn and multi-turn
        conversations and can be used for various tasks like question answering, code
        generation, and function calling.
      parameters:
      - description: Chat completion request
        in: body
        name: request
        required: true
        schema:
          $ref: '#/definitions/handler.ChatCompletionRequest'
      produces:
      - application/json
      responses:
        "200":
          description: Successful response for non-streaming requests.
          schema:
            $ref: '#/definitions/openai.ChatCompletion'
      summary: Creates a model response for the given chat conversation.
  /completions:
    post:
      consumes:
      - application/json
      description: Legacy completion endpoint for text generation. It is recommended
        to use the Chat Completions endpoint for new applications.
      parameters:
      - description: Completion request
        in: body
        name: request
        required: true
        schema:
          $ref: '#/definitions/openai.CompletionNewParams'
      produces:
      - application/json
      responses:
        "200":
          description: OK
          schema:
            $ref: '#/definitions/openai.Completion'
      summary: completion
  /embeddings:
    post:
      consumes:
      - application/json
      description: Creates an embedding for the given input.
      parameters:
      - description: Embedding request
        in: body
        name: request
        required: true
        schema:
          $ref: '#/definitions/openai.EmbeddingNewParams'
      responses: {}
      summary: Creates an embedding for the given input.
  /reranking:
    post:
      consumes:
      - application/json
      description: Reranks the given documents for the given query.
      parameters:
      - description: Reranking request
        in: body
        name: request
        required: true
        schema:
          $ref: '#/definitions/handler.RerankingRequest'
      responses: {}
      summary: Reranks the given documents for the given query.
swagger: "2.0"
