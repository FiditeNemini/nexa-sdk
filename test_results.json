{
    "results": {
        "openai_humaneval": {
            "alias": "openai_humaneval",
            "pass@1,none": 1.0,
            "pass@1_stderr,none": "N/A"
        }
    },
    "group_subtasks": {
        "openai_humaneval": []
    },
    "configs": {
        "openai_humaneval": {
            "task": "openai_humaneval",
            "tag": [
                "openai_humaneval"
            ],
            "dataset_path": "openai/openai_humaneval",
            "test_split": "test",
            "doc_to_text": "{{prompt}}",
            "doc_to_target": "{{canonical_solution}}",
            "process_results": "def process_results(doc: Dict[str, Any], results: List[str]) -> Dict[str, Any]:\n    completion = results[0]\n    result = check_correctness(doc, completion)\n    return {\n        \"pass@1\": int(result[\"passed\"]),\n    }\n",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "pass@1",
                    "aggregation": "def pass_at_1(samples: List[Dict[str, Any]]) -> float:\n    return pass_at_k(samples, 1)\n",
                    "higher_is_better": true
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "until": [
                    "\n\n",
                    "\nclass",
                    "\ndef"
                ],
                "do_sample": true,
                "temperature": 0.8,
                "top_p": 0.95
            },
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "openai_humaneval": 1.0
    },
    "n-shot": {
        "openai_humaneval": 0
    },
    "higher_is_better": {
        "openai_humaneval": {
            "pass@1": true
        }
    },
    "n-samples": {
        "openai_humaneval": {
            "original": 164,
            "effective": 164
        }
    },
    "config": {
        "model": "my-local-completions",
        "model_args": "base_url=http://0.0.0.0:8000/v1/completions",
        "batch_size": 8,
        "batch_sizes": [],
        "device": "cuda",
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 0,
        "numpy_seed": 1234,
        "torch_seed": 1234,
        "fewshot_seed": 1234
    },
    "git_hash": "b4c661a",
    "date": 1726663219.4638186,
    "pretty_env_info": "PyTorch version: 2.4.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 20.04.6 LTS (x86_64)\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\nClang version: Could not collect\nCMake version: version 3.30.3\nLibc version: glibc-2.31\n\nPython version: 3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:24:10) [GCC 9.4.0] (64-bit runtime)\nPython platform: Linux-5.15.0-1064-aws-x86_64-with-glibc2.31\nIs CUDA available: True\nCUDA runtime version: 12.1.105\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: Tesla T4\nNvidia driver version: 535.183.01\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nByte Order:                         Little Endian\nAddress sizes:                      46 bits physical, 48 bits virtual\nCPU(s):                             4\nOn-line CPU(s) list:                0-3\nThread(s) per core:                 2\nCore(s) per socket:                 2\nSocket(s):                          1\nNUMA node(s):                       1\nVendor ID:                          GenuineIntel\nCPU family:                         6\nModel:                              85\nModel name:                         Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\nStepping:                           7\nCPU MHz:                            2499.996\nBogoMIPS:                           4999.99\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          64 KiB\nL1i cache:                          64 KiB\nL2 cache:                           2 MiB\nL3 cache:                           35.8 MiB\nNUMA node0 CPU(s):                  0-3\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported\nVulnerability L1tf:                 Mitigation; PTE Inversion\nVulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Meltdown:             Mitigation; PTI\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\nVulnerability Retbleed:             Vulnerable\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Vulnerable\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Retpoline\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke avx512_vnni\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] onnx==1.16.2\n[pip3] onnxruntime==1.19.2\n[pip3] torch==2.4.1\n[pip3] triton==3.0.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.4.1                    pypi_0    pypi\n[conda] triton                    3.0.0                    pypi_0    pypi",
    "transformers_version": "4.44.2",
    "upper_git_hash": null
}