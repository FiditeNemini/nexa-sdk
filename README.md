<h1>Nexa SDK - Local On-Device Inference Framework</h1>
Nexa SDK is a comprehensive toolkit for supporting GGUF and MLX model formats. It supports LLM and VLMs.

Features
- Device Support: CPU, GPU (CUDA, Metal, Vulkan)
- Input Type Support: Text, Image, Audio
- Server: OpenAI-compatible API, JSON schema for function calling and streaming support
- Model Format Support: GGUF, MLX

# Latest News ðŸ”¥
- Beta launch for Nexa SDK, more updates coming soon!
Welcome to submit your requests through issues, we ship weekly.

# Installation
TODO 1

# Supported Models
TODO

# Run Models from ðŸ¤— HuggingFace

# CLI Reference
Here's a brief overview of the main CLI commands:
- nexa infer: Run inference for various tasks using GGUF models.

# Acknowledgements
We would like to thank the following projects:
- [llama.cpp](https://github.com/ggml-org/llama.cpp)
- [mlx-lm](https://github.com/ml-explore/mlx-lm)
- [mlx-vlm](https://github.com/Blaizzy/mlx-vlm)
- [mlx-audio](https://github.com/Blaizzy/mlx-audio)
