{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NexaAI NPU Inference Examples\n",
        "\n",
        "This notebook demonstrates how to use the NexaAI SDK for various AI inference tasks on NPU devices, including:\n",
        "\n",
        "- **LLM (Large Language Model)**: Text generation and conversation\n",
        "- **VLM (Vision Language Model)**: Multimodal understanding and generation\n",
        "- **Embedder**: Text vectorization and similarity computation\n",
        "- **Reranker**: Document reranking\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.10+\n",
        "- Windows ARM64 (Snapdragon X Elite) or NPU-capable device\n",
        "- NexaAI SDK installed: `pip install nexaai`\n",
        "\n",
        "## Model Download\n",
        "\n",
        "Before running the examples, download the corresponding NPU-optimized models:\n",
        "\n",
        "```bash\n",
        "# LLM model\n",
        "nexa pull NexaAI/Llama3.2-3B-NPU-Turbo\n",
        "\n",
        "# VLM model  \n",
        "nexa pull NexaAI/OmniNeural-4B\n",
        "\n",
        "# Embedder model\n",
        "nexa pull NexaAI/embeddinggemma-300m-npu\n",
        "\n",
        "# Reranker model\n",
        "nexa pull NexaAI/jina-v2-rerank-npu\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "\n",
        "# NexaAI SDK imports\n",
        "from nexaai.llm import LLM, GenerationConfig\n",
        "from nexaai.vlm import VLM\n",
        "from nexaai.embedder import Embedder, EmbeddingConfig\n",
        "from nexaai.rerank import Reranker, RerankConfig\n",
        "from nexaai.common import ModelConfig, ChatMessage, MultiModalMessage, MultiModalMessageContent\n",
        "\n",
        "print(\"NexaAI SDK imported successfully!\")\n",
        "print(\"Ready to start NPU inference examples...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. LLM (Large Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated large language models for text generation and conversation. Llama3.2-3B-NPU-Turbo is specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LLM NPU Inference Example\n",
        "def llm_npu_example():\n",
        "    \"\"\"LLM NPU inference example\"\"\"\n",
        "    print(\"=== LLM NPU Inference Example ===\")\n",
        "    \n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/Llama3.2-3B-NPU-Turbo\"\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant.\"\n",
        "    \n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    \n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    llm = LLM.from_(model_name, plugin_id=plugin_id, device_id=device, m_cfg=m_cfg)\n",
        "    \n",
        "    # Create conversation history\n",
        "    conversation = [ChatMessage(role=\"system\", content=system_message)]\n",
        "    \n",
        "    # Example conversations\n",
        "    test_prompts = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Explain the benefits of on-device AI processing.\",\n",
        "        \"How does NPU acceleration work?\"\n",
        "    ]\n",
        "    \n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\n--- Conversation {i} ---\")\n",
        "        print(f\"User: {prompt}\")\n",
        "        \n",
        "        # Add user message\n",
        "        conversation.append(ChatMessage(role=\"user\", content=prompt))\n",
        "        \n",
        "        # Apply chat template\n",
        "        formatted_prompt = llm.apply_chat_template(conversation)\n",
        "        \n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "        \n",
        "        for token in llm.generate_stream(formatted_prompt, g_cfg=GenerationConfig(max_tokens=max_tokens)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "        \n",
        "        # Get profiling data\n",
        "        profiling_data = llm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "        \n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(ChatMessage(role=\"assistant\", content=response_buffer.getvalue()))\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Run LLM example\n",
        "llm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. VLM (Vision Language Model) NPU Inference\n",
        "\n",
        "Using NPU-accelerated vision language models for multimodal understanding and generation. OmniNeural-4B supports joint processing of images and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VLM NPU Inference Example\n",
        "def vlm_npu_example():\n",
        "    \"\"\"VLM NPU inference example\"\"\"\n",
        "    print(\"=== VLM NPU Inference Example ===\")\n",
        "    \n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/OmniNeural-4B\"\n",
        "    plugin_id = \"npu\"\n",
        "    device = \"npu\"\n",
        "    max_tokens = 100\n",
        "    system_message = \"You are a helpful assistant that can understand images and text.\"\n",
        "    \n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    \n",
        "    # Create model instance\n",
        "    m_cfg = ModelConfig()\n",
        "    vlm = VLM.from_(name_or_path=model_name, m_cfg=m_cfg, plugin_id=plugin_id, device_id=device)\n",
        "    \n",
        "    # Create conversation history\n",
        "    conversation = [MultiModalMessage(role=\"system\", \n",
        "                                    content=[MultiModalMessageContent(type=\"text\", text=system_message)])]\n",
        "    \n",
        "    # Example multimodal conversations\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"text\": \"What do you see in this image?\",\n",
        "            \"image_path\": None  # Replace with actual image path if available\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Describe the main objects and their relationships.\",\n",
        "            \"image_path\": None\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"What is artificial intelligence and how does it relate to computer vision?\",\n",
        "            \"image_path\": None\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"\\n--- Multimodal Conversation {i} ---\")\n",
        "        print(f\"User: {case['text']}\")\n",
        "        \n",
        "        # Build message content\n",
        "        contents = [MultiModalMessageContent(type=\"text\", text=case['text'])]\n",
        "        \n",
        "        # Add image content if available\n",
        "        if case['image_path'] and os.path.exists(case['image_path']):\n",
        "            contents.append(MultiModalMessageContent(type=\"image\", text=case['image_path']))\n",
        "            print(f\"Including image: {case['image_path']}\")\n",
        "        \n",
        "        # Add user message\n",
        "        conversation.append(MultiModalMessage(role=\"user\", content=contents))\n",
        "        \n",
        "        # Apply chat template\n",
        "        formatted_prompt = vlm.apply_chat_template(conversation)\n",
        "        \n",
        "        # Generate response\n",
        "        print(\"Assistant: \", end=\"\", flush=True)\n",
        "        response_buffer = io.StringIO()\n",
        "        \n",
        "        # Prepare image and audio paths\n",
        "        image_paths = [case['image_path']] if case['image_path'] and os.path.exists(case['image_path']) else None\n",
        "        audio_paths = None\n",
        "        \n",
        "        for token in vlm.generate_stream(formatted_prompt, \n",
        "                                       g_cfg=GenerationConfig(max_tokens=max_tokens, \n",
        "                                                            image_paths=image_paths, \n",
        "                                                            audio_paths=audio_paths)):\n",
        "            print(token, end=\"\", flush=True)\n",
        "            response_buffer.write(token)\n",
        "        \n",
        "        # Get profiling data\n",
        "        profiling_data = vlm.get_profiling_data()\n",
        "        if profiling_data:\n",
        "            print(f\"\\nProfiling data: {profiling_data}\")\n",
        "        \n",
        "        # Add assistant response to conversation history\n",
        "        conversation.append(MultiModalMessage(role=\"assistant\", \n",
        "                                            content=[MultiModalMessageContent(type=\"text\", text=response_buffer.getvalue())]))\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Run VLM example\n",
        "vlm_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedder NPU Inference\n",
        "\n",
        "Using NPU-accelerated embedding models for text vectorization and similarity computation. embeddinggemma-300m-npu is a lightweight embedding model specifically optimized for NPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedder NPU Inference Example\n",
        "def embedder_npu_example():\n",
        "    \"\"\"Embedder NPU inference example\"\"\"\n",
        "    print(\"=== Embedder NPU Inference Example ===\")\n",
        "    \n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/embeddinggemma-300m-npu\"\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 2\n",
        "    \n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    \n",
        "    # Create embedder instance\n",
        "    embedder = Embedder.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Embedder loaded successfully!')\n",
        "    \n",
        "    # Get embedding dimension\n",
        "    dim = embedder.get_embedding_dim()\n",
        "    print(f\"Embedding dimension: {dim}\")\n",
        "    \n",
        "    # Example texts\n",
        "    texts = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"Nexa AI allows you to run state-of-the-art AI models locally on CPU, GPU, or NPU.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads.\"\n",
        "    ]\n",
        "    \n",
        "    query = \"what is on device AI\"\n",
        "    \n",
        "    print(f\"\\n=== Generating Embeddings ===\")\n",
        "    print(f\"Processing {len(texts)} texts...\")\n",
        "    \n",
        "    # Generate embeddings\n",
        "    embeddings = embedder.generate(\n",
        "        texts=texts, \n",
        "        config=EmbeddingConfig(batch_size=batch_size)\n",
        "    )\n",
        "    \n",
        "    print(f\"Successfully generated {len(embeddings)} embeddings\")\n",
        "    \n",
        "    # Display embedding information\n",
        "    print(f\"\\n=== Embedding Details ===\")\n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        print(f\"\\nText {i+1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Embedding dimension: {len(embedding)}\")\n",
        "        print(f\"  First 10 elements: {embedding[:10]}\")\n",
        "        print(\"-\" * 70)\n",
        "    \n",
        "    # Query processing\n",
        "    print(f\"\\n=== Query Processing ===\")\n",
        "    print(f\"Query: '{query}'\")\n",
        "    \n",
        "    query_embedding = embedder.generate(\n",
        "        texts=[query], \n",
        "        config=EmbeddingConfig(batch_size=1)\n",
        "    )[0]\n",
        "    \n",
        "    print(f\"Query embedding dimension: {len(query_embedding)}\")\n",
        "    \n",
        "    # Similarity analysis\n",
        "    print(f\"\\n=== Similarity Analysis (Inner Product) ===\")\n",
        "    similarities = []\n",
        "    \n",
        "    for i, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
        "        query_vec = np.array(query_embedding)\n",
        "        text_vec = np.array(embedding)\n",
        "        inner_product = np.dot(query_vec, text_vec)\n",
        "        similarities.append((i, text, inner_product))\n",
        "        \n",
        "        print(f\"\\nText {i+1}:\")\n",
        "        print(f\"  Content: {text}\")\n",
        "        print(f\"  Inner product with query: {inner_product:.6f}\")\n",
        "        print(\"-\" * 70)\n",
        "    \n",
        "    # Sort and display most similar texts\n",
        "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "    \n",
        "    print(f\"\\n=== Similarity Ranking Results ===\")\n",
        "    for rank, (idx, text, score) in enumerate(similarities, 1):\n",
        "        print(f\"Rank {rank}: [{score:.6f}] {text}\")\n",
        "    \n",
        "    return embeddings, query_embedding, similarities\n",
        "\n",
        "# Run Embedder example\n",
        "embeddings, query_emb, similarities = embedder_npu_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Reranker NPU Inference\n",
        "\n",
        "Using NPU-accelerated reranking models for document reranking. jina-v2-rerank-npu can perform precise similarity-based document ranking based on queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reranker NPU Inference Example\n",
        "def reranker_npu_example():\n",
        "    \"\"\"Reranker NPU inference example\"\"\"\n",
        "    print(\"=== Reranker NPU Inference Example ===\")\n",
        "    \n",
        "    # Model configuration\n",
        "    model_name = \"NexaAI/jina-v2-rerank-npu\"\n",
        "    plugin_id = \"npu\"\n",
        "    batch_size = 4\n",
        "    \n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(f\"Using plugin: {plugin_id}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    \n",
        "    # Create reranker instance\n",
        "    reranker = Reranker.from_(name_or_path=model_name, plugin_id=plugin_id)\n",
        "    print('Reranker loaded successfully!')\n",
        "    \n",
        "    # Example queries and documents\n",
        "    queries = [\n",
        "        \"Where is on-device AI?\",\n",
        "        \"What is NPU acceleration?\",\n",
        "        \"How does machine learning work?\",\n",
        "        \"Tell me about computer vision\"\n",
        "    ]\n",
        "    \n",
        "    documents = [\n",
        "        \"On-device AI is a type of AI that is processed on the device itself, rather than in the cloud.\",\n",
        "        \"NPU acceleration provides significant performance improvements for AI workloads on specialized hardware.\",\n",
        "        \"Edge computing brings computation and data storage closer to the sources of data.\",\n",
        "        \"A ragdoll is a breed of cat that is known for its long, flowing hair and gentle personality.\",\n",
        "        \"The capital of France is Paris, a beautiful city known for its art and culture.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n",
        "        \"Computer vision is a field of artificial intelligence that trains computers to interpret and understand visual information.\",\n",
        "        \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"\\n=== Document Reranking Test ===\")\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "    \n",
        "    # Rerank for each query\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\n--- Query {i} ---\")\n",
        "        print(f\"Query: '{query}'\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Perform reranking\n",
        "        scores = reranker.rerank(\n",
        "            query=query, \n",
        "            documents=documents, \n",
        "            config=RerankConfig(batch_size=batch_size)\n",
        "        )\n",
        "        \n",
        "        # Create (document, score) pairs and sort\n",
        "        doc_scores = list(zip(documents, scores))\n",
        "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # Display ranking results\n",
        "        print(\"Reranking results:\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores, 1):\n",
        "            print(f\"  {rank:2d}. [{score:.4f}] {doc}\")\n",
        "        \n",
        "        # Display most relevant documents\n",
        "        print(f\"\\nMost relevant documents (top 3):\")\n",
        "        for rank, (doc, score) in enumerate(doc_scores[:3], 1):\n",
        "            print(f\"  {rank}. {doc}\")\n",
        "        \n",
        "        print(\"=\" * 80)\n",
        "    \n",
        "    return reranker\n",
        "\n",
        "# Run Reranker example\n",
        "reranker = reranker_npu_example()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
